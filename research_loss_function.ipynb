{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpdFVa+HJgp28Dbck/ksdX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sedavasema/mohammad/blob/main/research_loss_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "_5vmIMfonc7n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Cross Entropy\n",
        "The Binary Cross entropy will calculate the cross-entropy loss between the predicted classes and the true classes. By default, the sum_over_batch_size reduction is used. This means that the loss will return the average of the per-sample losses in the batch."
      ],
      "metadata": {
        "id": "b3Yb2AIiqCA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [[0., 1.], [0.2, 0.8],[0.3, 0.7],[0.4, 0.6]]\n",
        "y_pred = [[0.6, 0.4], [0.4, 0.6],[0.6, 0.4],[0.8, 0.2]]\n",
        "bce = tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size')\n",
        "bce(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "id": "QfRIJ-q8ntvM",
        "outputId": "79a845a1-b93f-4d46-d49e-6028b64f52c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.839445"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sum reduction means that the loss function will return the sum of the per-sample losses in the batch."
      ],
      "metadata": {
        "id": "46Pn-v6IqMIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bce = tf.keras.losses.BinaryCrossentropy(reduction='sum')\n",
        "bce(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2yJioOvpP2U",
        "outputId": "8d7eeedc-0f40-4ba1-86f0-4c9b050bdf29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.35778"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the reduction as none returns the full array of the per-sample losses."
      ],
      "metadata": {
        "id": "dXKzrj2_qmKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n",
        "bce(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_8eX_PcqngC",
        "outputId": "15a35583-44b8-4066-ea18-e64dc48c67da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9162905 , 0.5919184 , 0.79465103, 1.0549198 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Crossentropy\n",
        "The CategoricalCrossentropy also computes the cross-entropy loss between the true classes and predicted classes. The labels are given in an one_hot format."
      ],
      "metadata": {
        "id": "tMsLr_bFtoHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "cce(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx7RM-Ictqn2",
        "outputId": "1549d683-a062-4b55-cae2-452a76003d4c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83944523"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Categorical Crossentropy\n",
        "If you have two or more classes and  the labels are integers, the SparseCategoricalCrossentropy should be used."
      ],
      "metadata": {
        "id": "kx4hfuC3uJoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [0, 1,2]\n",
        "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],[0.1, 0.8, 0.1]]\n",
        "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "scce(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzwQ-78quPTs",
        "outputId": "4ebaf863-0c17-4603-98bb-064d8cf0ade3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.840487"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Poison Loss\n",
        "You can also use the Poisson class to compute the poison loss. It’s a great choice if your dataset comes from a Poisson distribution for example the number of calls a call center receives per hour."
      ],
      "metadata": {
        "id": "7mDyz_Ecuxut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [[0.1, 1.,0.8], [0.1, 0.9,0.1],[0.2, 0.7,0.1],[0.3, 0.1,0.6]]\n",
        "y_pred = [[0.6, 0.2,0.2], [0.2, 0.6,0.2],[0.7, 0.1,0.2],[0.8, 0.1,0.1]]\n",
        "p = tf.keras.losses.Poisson()\n",
        "p(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHEtCDsVu1rz",
        "outputId": "4a233b55-518a-4f88-9f93-311198c3d687"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9377117"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kullback-Leibler Divergence Loss\n",
        "The relative entropy can be computed using the KLDivergence class. According to the official docs at PyTorch:\n",
        "\n",
        "KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions."
      ],
      "metadata": {
        "id": "TkIEKF4rvAwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [[0.1, 1.,0.8], [0.1, 0.9,0.1],[0.2, 0.7,0.1],[0.3, 0.1,0.6]]\n",
        "y_pred = [[0.6, 0.2,0.2], [0.2, 0.6,0.2],[0.7, 0.1,0.2],[0.8, 0.1,0.1]]\n",
        "kl = tf.keras.losses.KLDivergence()\n",
        "kl(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SLbyGJxvEOI",
        "outputId": "5815f5ce-5734-47b3-9a79-4b6ecc2ca02e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1471658"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Detection\n",
        "# The Focal Loss\n",
        "In classification problems involving imbalanced data and object detection problems, you can use the Focal Loss. The loss introduces an adjustment to the cross-entropy criterion."
      ],
      "metadata": {
        "id": "WkEfBZLPvoTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It is done by altering its shape in a way that the loss allocated to well-classified examples is down-weighted. This ensures that the model is able to learn equally from minority and majority classes.\n",
        "\n",
        "![0_8fyhgPJALUUKHKrr.png](data:image/png;base64,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)\n",
        "\n",
        "The cross-entropy loss is scaled by scaling the factors decaying at zero as the confidence in the correct class increases. The factor of scaling down weights the contribution of unchallenging samples at training time and focuses on the challenging ones."
      ],
      "metadata": {
        "id": "tPt-Y4OSv-57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.addons as tfa\n",
        "\n",
        "y_true = [[0.97], [0.91], [0.03]]\n",
        "y_pred = [[1.0], [1.0], [0.0]]\n",
        "sfc = tfa.losses.SigmoidFocalCrossEntropy()\n",
        "sfc(y_true, y_pred).numpy()\n",
        "array([0.00010971, 0.00329749, 0.00030611], dtype=float32)"
      ],
      "metadata": {
        "id": "FZbtUmVcvrPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "In regression problems, you have to calculate the differences between the predicted values and the true values but as always there are many ways to do it.\n",
        "\n",
        "# Mean Sqre Error\n",
        "The MeanSquaredError class can be used to compute the mean square of errors between the predictions and the true values."
      ],
      "metadata": {
        "id": "b1m8i8v2w9Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [12, 20, 29., 60.]\n",
        "y_pred = [14., 18., 27., 55.]\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "mse(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrDMBUczxATD",
        "outputId": "c499b9d3-85a3-4e85-8656-66c37c1ef264"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.25"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Percentage Error\n",
        "The mean absolute percentage error is computed using the function below.\n",
        "\n",
        "![gif.gif](data:image/gif;base64,R0lGODlhQwEYALMAAP///wAAAKqqqjIyMhAQEHZ2dtzc3O7u7mZmZiIiIrq6upiYmFRUVIiIiMzMzERERCH5BAEAAAAALAAAAABDARgAAAT+EMhJq7046827/2AIGGJpnujmeAeZvnAsz7SmFHWuiw7OFYIdRjAg+ISzA0NxaRQaCGbFCZUKGYmAyPBAer/TBScBrhgCwTKKgWAErBNGY/JIS+R0+44xED1caoE5AwcaDgyCAAIBhYkmDm8VCloTCgSVlBKWXkYhCoiOoTACoBhRiQh9ojyRFKkVra8UrTpncB0MK6u7JgSNF2SJCUe8HZC3BKUSAT7JsMQ1DZkfl8XWH3gXCgiJthYLTwh6AAsNTrfl50LHsNwUCV0AAe4T8BoHwRsF+woOvw8Pyj2pkK6AlQXxKOwjVufahYUUGu5AiCFXBYh5aCyYdmeOhAT+doBMUCVBpASSOdhNODCvQoI+LOl9RHmhWoYDA6RIE7MsgQuQE0wCILlvyohM3hxSmHNmQlIdDmy6NNpUwtMYfCowUFaAZCYBCeVNAJuh64CzaNOevVVBpYSYLmG2fEdTwgIGhcgceMAWwAN6k1y43Trha8JsVsVsHMvRoQHFXxvTqFpBALHH5CLXIOBRUwBAAArYDMD3Aum+NdwukwmAgKq5E1xfOFAgwSEBwy4sAiR6QgOpD7yWrjCApwRdD0rJUnpcQvIJy19wCQuAJWjnoJErr5vijC7n+ZyTlBYAtgTy5lPSWg0rXnrSNxF0wvAg/IBSACk4O1/ePNAKLNn+kZtV1M2QwIEIJnggd2+hUQ80KThhwWcAchfggwTCsJh+rKW3jYOSIAAiVOu1pow8zZzIDAa0MQBQAb9wOItxnDkVQGeaiGhHcd9kwtJBrBWzYXUlpjDcSowQZBwFQ/5oV5AluFhdYUtutIIAMgUkAZYRLXmRWmCuZUiJ0TkJQJlFKnKjX6EF8F2DxkFCwgq0/FYIl3QYh1gcJC3yCwPjFJPVlknKIJVVjRFywaBqNgIoDDUeQcCSA/hwHwUDuHDpSNfRoBoAk1AggE2hjnWoUy5U48+EIV1ygBgEfJeAR5ueBAgCED4nAQLBKCBNAWJgaYAADUhBbBoLWNEDsTn+6HpmMAsgMCw4W0pLrBTLduZAA8TW5UB4IxTonHJk+MpMsNYWuyW3dikLxBwEBHFEA8rRQ0w5QTGJYw6TeAkAA0fkRAHAI6GGpAYEKxIcAA0UggBPCNirL5MFFhAPJPQYUI0CB7TxrxgOPObOALo4oIoAb8Zg8XEtKWAAAvE84ADHHjMA8skrKBAPrt9Qx+5DF8+lsSYdI2IzACFHe1LJOJPS8FLh6HFAAwuUIzXVVgtRABYBJMMaFeJYAHagut2DgDkryGHH2Q9TMHXVP9u46D4b2XFXcyRTQMKjYkkQL9IQYkW3gyvkPdQChb/5dw8fSXFkHOMweMfgyIKSuN7+/6aRyeKBM+e5EIbrNk3bDdakiSpnCOCyF4vMEtsKLOmNxupIbx7jR22dOEQmpBNpOqioz97p58TrQAqTR/z1zpygFmhySTAeoAC4GiUP2MW9hjV9Zao8D9rxrqBGrXP0+IR08xY8H1r02xfv/heK+u2OJTFqYQATA0kCSk5B4FNJyi8gwPx8ERRQDCd/b8lHPxywMwQcQA+ZmgoGBKgJAi5jBPjbF6j2p4D+KRCA7wthd5STwJQZRAoMGJ44BHCsdXUQhClwBz7eNABuqesO17kWykpSNQf0jmFe2kYGZGgbhSgAhSpkYQsVUawdivCJMfDVcYBVgOF5rgfguI4kZFLAOK3AEHBZhKIYrzhGDqhvBiD8YhnXyEYoMjABBmujBSIAADs=)\n",
        "\n",
        "It is calculated as shown below.\n",
        "\n",
        "Consider using this loss when you want a loss that you can explain intuitively. People understand percentages easily. The loss is also robust to outliers."
      ],
      "metadata": {
        "id": "3EJZ5oKJzcjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [12, 20, 29., 60.]\n",
        "y_pred = [14., 18., 27., 55.]\n",
        "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "mape(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss41gADlztAG",
        "outputId": "c6617d17-3bc8-4daf-93c1-c81c06bcdbe0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.474138"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Similarity Loss\n",
        "If your interest is in computing the cosine similarity between the true and predicted values, you’d use the CosineSimilarity class. It is computed as:\n",
        "\n",
        "\n",
        "The result is a number between  -1 and 1 . 0 indicates orthogonality while values close to -1 show that there is great similarity."
      ],
      "metadata": {
        "id": "raE9WUiE0h3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [[12, 20], [29., 60.]]\n",
        "y_pred = [[14., 18.], [27., 55.]]\n",
        "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
        "cosine_loss(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nci6sV1d0juE",
        "outputId": "65b4e7db-fe6b-448b-f2ae-e0e5a34da977"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9963575"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Embeddings\n",
        "# Triplet Loss\n",
        "You can also compute the triplet loss with semi-hard negative mining via TensorFlow addons. The loss encourages the positive distances between pairs of embeddings with the same labels to be less than the minimum negative distance."
      ],
      "metadata": {
        "id": "Rbwm2r1I032u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tfa.losses.TripletSemiHardLoss(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4OL1xDDy08i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating custom loss functions in Keras\n",
        "Sometimes there is no good loss available or you need to implement some modifications. Let’s learn how to do that.\n",
        "\n",
        "A custom loss function can be created by defining a function that takes the true values and predicted values as required parameters. The function should return an array of losses. The function can then be passed at the compile stage."
      ],
      "metadata": {
        "id": "IFZ2-GmY1RE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_function(y_true, y_pred):\n",
        "   squared_difference = tf.square(y_true - y_pred)\n",
        "   return tf.reduce_mean(squared_difference, axis=-1)\n",
        "\n",
        "model.compile(optimizer='adam', loss=custom_loss_function)"
      ],
      "metadata": {
        "id": "BXFjf66i1VH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see how we can apply this custom loss function to an array of predicted and true values."
      ],
      "metadata": {
        "id": "mA8foaGv1pB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = [12, 20, 29., 60.]\n",
        "y_pred = [14., 18., 27., 55.]\n",
        "cl = custom_loss_function(np.array(y_true),np.array(y_pred))\n",
        "cl.numpy()"
      ],
      "metadata": {
        "id": "mvhoF_o71xrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use of Keras loss weights\n",
        "During the training process, one can weigh the loss function by observations or samples. The weights can be arbitrary, but a typical choice is class weights (distribution of labels). Each observation is weighted by the fraction of the class it belongs to (reversed) so that the loss for minority class observations is more important when calculating the loss.\n",
        "\n",
        "One of the ways to do this is to pass the class weights during the training process.\n",
        "\n",
        "The weights are passed using a dictionary that contains the weight for each class. You can compute the weights using Scikit-learn or calculate the weights based on your own criterion."
      ],
      "metadata": {
        "id": "0iNi5zq32G_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = { 0:1.01300017,1:0.88994364,2:1.00704935, 3:0.97863318,      4:1.02704553, 5:1.10680686,6:1.01385603,7:0.95770152, 8:1.02546573,\n",
        "               9:1.00857287}\n",
        "model.fit(x_train, y_train,verbose=1, epochs=10,class_weight=weights)"
      ],
      "metadata": {
        "id": "O9awSvZs2Jjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second way is to pass these weights at the compile stage."
      ],
      "metadata": {
        "id": "-5ndevYR2RxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [1.013, 0.889, 1.007, 0.978, 1.027,1.106,1.013,0.957,1.025, 1.008]\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              loss_weights=weights,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VZKa3c4u2TMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}